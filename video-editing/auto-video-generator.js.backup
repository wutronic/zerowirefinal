#!/usr/bin/env node

const fs = require('fs');
const path = require('path');
const chokidar = require('chokidar');
const { exec } = require('child_process');
const { promisify } = require('util');
const minimist = require('minimist');

const execAsync = promisify(exec);

// Configuration - Use absolute paths based on this file's location
const SCRIPT_DIR = path.dirname(__filename);  // video-editing directory
const PROJECT_ROOT = path.dirname(SCRIPT_DIR); // zerowirefinal directory

const CONFIG = {
    audioWatchFolder: path.join(PROJECT_ROOT, 'zero-wire/Spark-TTS/audiooutput/done'),
    videoTemplatesBase: path.join(PROJECT_ROOT, 'VideoTemplates/style 1'),
    outputFolder: path.join(PROJECT_ROOT, 'generated-videos'),
    finalOutputFolder: path.join(PROJECT_ROOT, 'FinalOutput'),  // New: Final output for split-screen videos
    splitscreenFolder: path.join(PROJECT_ROOT, 'VideoTemplates/style 1/splitscreen'),
    splitscreenSourceFolder: path.join(PROJECT_ROOT, 'splitscreensource'),
    supportedAudioFormats: ['.wav', '.mp3', '.m4a', '.aac'],
    supportedVideoFormats: ['.mp4', '.mov', '.avi'],
    videoPaddingDuration: 1.0  // Add 1 second of video padding after audio ends
};

// Split screen configuration for overlay positioning
const SPLIT_SCREEN_CONFIG = {
    // Template video positioned at 25% from top (center), cropped to 50% height
    templateVideo: {
        x: 0.0,      // Full width
        y: 0.125,    // 12.5% from top (25% center - 12.5% half height = 12.5%)
        width: 1.0,  // 100% of screen width
        height: 0.25, // 25% of screen height (50% cropped)
        cropY: 0.25, // Crop 25% from top
        cropHeight: 0.5 // Use middle 50% of video
    },
    // User uploaded video positioned at 75% from top (center), cropped to 50% height  
    userVideo: {
        x: 0.0,      // Full width
        y: 0.625,    // 62.5% from top (75% center - 12.5% half height = 62.5%)
        width: 1.0,  // 100% of screen width
        height: 0.25, // 25% of screen height (50% cropped)
        cropY: 0.25, // Crop 25% from top
        cropHeight: 0.5 // Use middle 50% of video
    }
};

// Ensure output folder exists
if (!fs.existsSync(CONFIG.outputFolder)) {
    fs.mkdirSync(CONFIG.outputFolder, { recursive: true });
}

/**
 * Atomic write helper function to prevent race conditions
 */
async function writeStatusFileAtomically(filePath, data) {
    const tempPath = filePath + '.tmp';
    const content = JSON.stringify(data, null, 2);
    
    try {
        await fs.promises.writeFile(tempPath, content, 'utf-8');
        await fs.promises.rename(tempPath, filePath);
        console.log(`üîí Atomic write completed: ${path.basename(filePath)}`);
    } catch (error) {
        console.error(`‚ùå Atomic write failed: ${error.message}`);
        // Clean up temp file if it exists
        try {
            await fs.promises.unlink(tempPath);
        } catch (unlinkError) {
            // Ignore cleanup errors
        }
        throw error;
    }
}

/**
 * Safe JSON parse with retry logic for handling race conditions
 */
function safeJsonParse(content, retries = 3, baseDelay = 100) {
    for (let i = 0; i < retries; i++) {
        try {
            return JSON.parse(content);
        } catch (error) {
            if (i === retries - 1) {
                console.error(`‚ùå JSON parse failed after ${retries} attempts: ${error.message}`);
                throw error;
            }
            console.log(`‚ö†Ô∏è JSON parse attempt ${i + 1} failed, retrying...`);
            // Synchronous delay for retry
            const delay = baseDelay * Math.pow(2, i);
            const start = Date.now();
            while (Date.now() - start < delay) {
                // Busy wait
            }
        }
    }
}

/**
 * Safe file read with retry logic for status files
 */
async function readStatusFileWithRetry(filePath, maxRetries = 5) {
    for (let i = 0; i < maxRetries; i++) {
        try {
            const content = await fs.promises.readFile(filePath, 'utf-8');
            return safeJsonParse(content);
        } catch (error) {
            if (error.code === 'ENOENT') {
                throw error; // File doesn't exist, don't retry
            }
            if (i === maxRetries - 1) {
                console.error(`‚ùå File read failed after ${maxRetries} attempts: ${error.message}`);
                throw error;
            }
            console.log(`‚ö†Ô∏è File read attempt ${i + 1} failed, retrying in ${200 * (i + 1)}ms...`);
            await new Promise(resolve => setTimeout(resolve, 200 * (i + 1)));
        }
    }
}

/**
 * Generate debug text for a clip
 */
function generateDebugText(clipType, clipIndex, fullDuration, usedDuration, transitionType = 'dummy') {
    const fullDur = fullDuration.toFixed(2);
    const usedDur = usedDuration.toFixed(2);
    const clipName = clipType === 'loop' ? `${clipType.toUpperCase()}${clipIndex}` : clipType.toUpperCase();
    
    return `${clipName} | Full: ${fullDur}s | Used: ${usedDur}s | Trans: ${transitionType}`;
}

/**
 * Get duration of media file using ffprobe
 */
async function getMediaDuration(filePath) {
    try {
        const { stdout } = await execAsync(`ffprobe -v quiet -show_entries format=duration -of csv=p=0 "${filePath}"`);
        return parseFloat(stdout.trim());
    } catch (error) {
        console.error(`Error getting duration for ${filePath}:`, error.message);
        return 0;
    }
}

/**
 * Get video dimensions using ffprobe
 */
async function getVideoDimensions(filePath) {
    try {
        const { stdout } = await execAsync(`ffprobe -v quiet -select_streams v:0 -show_entries stream=width,height -of csv=p=0 "${filePath}"`);
        const [width, height] = stdout.trim().split(',').map(Number);
        return { width, height };
    } catch (error) {
        console.error(`Error getting dimensions for ${filePath}:`, error.message);
        return { width: 1920, height: 1080 }; // fallback
    }
}

/**
 * Get random file from directory with supported extensions
 */
function getRandomVideoFile(directory, extensions = CONFIG.supportedVideoFormats) {
    console.log(`\nüîç === ROBUST DIRECTORY DEBUGGING ===`);
    console.log(`üéØ Looking for directory: ${directory}`);
    console.log(`üìÇ Current working directory: ${process.cwd()}`);
    console.log(`üìÇ Absolute path: ${path.resolve(directory)}`);
    
    // Check if directory exists
    if (!fs.existsSync(directory)) {
        console.error(`‚ùå Directory not found: ${directory}`);
        console.log(`üîç Let's check what exists around this path...`);
        
        // Check parent directory
        const parentDir = path.dirname(directory);
        console.log(`üìÅ Parent directory: ${parentDir}`);
        if (fs.existsSync(parentDir)) {
            console.log(`‚úÖ Parent exists. Contents:`);
            try {
                const parentContents = fs.readdirSync(parentDir);
                parentContents.forEach(item => {
                    const itemPath = path.join(parentDir, item);
                    const stats = fs.statSync(itemPath);
                    console.log(`   ${stats.isDirectory() ? 'üìÅ' : 'üìÑ'} ${item}`);
                });
            } catch (error) {
                console.error(`‚ùå Error reading parent: ${error.message}`);
            }
        } else {
            console.log(`‚ùå Parent directory also doesn't exist`);
            
            // Let's check VideoTemplates base
            console.log(`üîç Checking VideoTemplates base...`);
            if (fs.existsSync('VideoTemplates')) {
                console.log(`‚úÖ VideoTemplates found. Contents:`);
                const contents = fs.readdirSync('VideoTemplates');
                contents.forEach(item => {
                    const itemPath = path.join('VideoTemplates', item);
                    const stats = fs.statSync(itemPath);
                    console.log(`   ${stats.isDirectory() ? 'üìÅ' : 'üìÑ'} ${item}`);
                });
            } else {
                console.log(`‚ùå VideoTemplates directory not found`);
            }
        }
        return null;
    }
    
    console.log(`‚úÖ Directory found: ${directory}`);
    
    const files = fs.readdirSync(directory)
        .filter(file => extensions.some(ext => file.toLowerCase().endsWith(ext)))
        .filter(file => !file.startsWith('.'));
    
    console.log(`üìÑ Found ${files.length} video files:`, files);
    
    if (files.length === 0) {
        console.error(`‚ùå No video files found in: ${directory}`);
        return null;
    }
    
    const randomFile = files[Math.floor(Math.random() * files.length)];
    const fullPath = path.join(directory, randomFile);
    console.log(`üéØ Selected: ${randomFile}`);
    console.log(`üìç Full path: ${fullPath}`);
    console.log(`===============================\n`);
    
    return fullPath;
}

/**
 * Process status file for API coordination
 */
async function processStatusFile(statusFilePath, debugOverlay = false, splitScreenMode = false) {
    try {
        console.log(`üîÑ Processing status file: ${path.basename(statusFilePath)}`);
        
        // üö® CRITICAL WORKING DIRECTORY DEBUGGING
        console.log(`\nüîç === CRITICAL WORKING DIRECTORY DEBUG ===`);
        console.log(`üìÇ Current working directory: ${process.cwd()}`);
        console.log(`üìç Status file path: ${statusFilePath}`);
        console.log(`üìç Status file absolute: ${path.resolve(statusFilePath)}`);
        console.log(`üìã CONFIG.videoTemplatesBase: "${CONFIG.videoTemplatesBase}"`);
        console.log(`üìã CONFIG.videoTemplatesBase absolute: "${path.resolve(CONFIG.videoTemplatesBase)}"`);
        
        // Test if our VideoTemplates directory exists from this working directory
        const testPaths = [
            'VideoTemplates',
            './VideoTemplates', 
            'VideoTemplates/style 1',
            './VideoTemplates/style 1',
            'VideoTemplates/style 1/Intro',
            './VideoTemplates/style 1/Intro'
        ];
        
        console.log(`\nüß™ Testing paths from current working directory:`);
        testPaths.forEach(testPath => {
            const exists = fs.existsSync(testPath);
            const resolved = path.resolve(testPath);
            console.log(`   ${exists ? '‚úÖ' : '‚ùå'} ${testPath} -> ${resolved}`);
        });
        
        // Check what actually exists in current directory
        console.log(`\nüìÅ Current directory contents:`);
        try {
            const contents = fs.readdirSync('.');
            contents.slice(0, 10).forEach(item => { // Show first 10 items
                const stats = fs.statSync(item);
                console.log(`   ${stats.isDirectory() ? 'üìÅ' : 'üìÑ'} ${item}`);
            });
            if (contents.length > 10) {
                console.log(`   ... and ${contents.length - 10} more items`);
            }
        } catch (error) {
            console.error(`   ‚ùå Error reading current directory: ${error.message}`);
        }
        console.log(`===========================================\n`);
        
        // Read and parse status file safely
        const statusContent = fs.readFileSync(statusFilePath, 'utf-8');
        const statusData = safeJsonParse(statusContent);
        
        if (statusData.status !== 'audio_ready') {
            console.log(`‚ö†Ô∏è Status file not ready: ${statusData.status}`);
            return;
        }
        
        // üêõ DEBUG: Log all status data for split screen debugging
        console.log(`\nüîç DEBUG: Status file analysis:`);
        console.log(`   - Request ID: ${statusData.requestId}`);
        console.log(`   - Split Screen Enabled: ${statusData.splitScreenEnabled}`);
        console.log(`   - Split Screen Path: ${statusData.splitScreenPath}`);
        console.log(`   - Audio File: ${statusData.audioFile}`);
        console.log(`   - Debug Mode: ${statusData.debugMode}`);
        
        // Update status to video_processing
        statusData.status = 'video_processing';
        statusData.videoStartTime = new Date().toISOString();
        await writeStatusFileAtomically(statusFilePath, statusData);
        console.log(`üìã Status updated: video_processing`);
        
        // üîÄ SPLIT SCREEN DETECTION AND PROCESSING
        if (statusData.splitScreenEnabled) {
            console.log(`\nüîÄ SPLIT SCREEN MODE DETECTED!`);
            
            // Check if a specific split screen file was uploaded
            if (statusData.splitScreenPath && fs.existsSync(statusData.splitScreenPath)) {
                console.log(`üìÅ Using uploaded split screen file: ${statusData.splitScreenPath}`);
                
                // Get file info for debugging
                const stats = fs.statSync(statusData.splitScreenPath);
                console.log(`üìä File size: ${(stats.size / (1024 * 1024)).toFixed(2)} MB`);
                
                // Get video properties for debugging
                try {
                    const duration = await getMediaDuration(statusData.splitScreenPath);
                    const dimensions = await getVideoDimensions(statusData.splitScreenPath);
                    console.log(`‚è±Ô∏è Duration: ${duration.toFixed(2)}s, Dimensions: ${dimensions.width}x${dimensions.height}`);
                } catch (error) {
                    console.warn(`‚ö†Ô∏è Could not get video properties: ${error.message}`);
                }
            } else {
                // No uploaded file - use random video from splitscreen folder
                console.log(`üìÅ No uploaded file - using random split screen video from folder`);
                statusData.splitScreenPath = null; // Will be handled by generateSplitScreenVideo
            }
            
            const outputPath = await generateSplitScreenVideo(statusData, statusFilePath);
            
            if (outputPath && fs.existsSync(outputPath)) {
                console.log(`‚úÖ Split screen video generated successfully!`);
                console.log(`üìÅ Output: ${path.basename(outputPath)}`);
                
                // Update status to complete
                statusData.status = 'video_complete';
                statusData.videoFile = path.basename(outputPath);
                statusData.videoCompleteTime = new Date().toISOString();
                statusData.coordinationMethod = 'file_watcher';
                
                await writeStatusFileAtomically(statusFilePath, statusData);
                console.log(`üéâ Split screen processing complete: ${path.basename(outputPath)}`);
                
                return outputPath;
            } else {
                throw new Error('Split screen video generation failed');
            }
        }
        
        // üé¨ NORMAL VIDEO PROCESSING (non-split screen)
        else {
            console.log(`\nüé¨ Normal video mode (no split screen)`);
            
            // Extract audio file path and resolve it properly
            const audioFile = path.join(PROJECT_ROOT, 'zero-wire/Spark-TTS', statusData.audioFile);
            
            if (!fs.existsSync(audioFile)) {
                throw new Error(`Audio file not found: ${audioFile}`);
            }
            
            console.log(`üéµ Processing audio file: ${path.basename(audioFile)}`);
            
            const outputPath = await processAudioFile(audioFile, statusData.debugMode || false, false, null);
            
            if (outputPath && fs.existsSync(outputPath)) {
                console.log(`‚úÖ Normal video generated successfully!`);
                console.log(`üìÅ Output: ${path.basename(outputPath)}`);
                
                // Update status to complete
                statusData.status = 'video_complete';
                statusData.videoFile = path.basename(outputPath);
                statusData.videoCompleteTime = new Date().toISOString();
                statusData.coordinationMethod = 'file_watcher';
                
                await writeStatusFileAtomically(statusFilePath, statusData);
                console.log(`üéâ Normal video processing complete: ${path.basename(outputPath)}`);
                
                return outputPath;
            } else {
                throw new Error('Normal video generation failed');
            }
        }
        
    } catch (error) {
        console.error(`‚ùå Status file processing failed:`, error.message);
        
        // Update status to error
        try {
            const statusContent = fs.readFileSync(statusFilePath, 'utf-8');
            const statusData = safeJsonParse(statusContent);
            
            statusData.status = 'error';
            statusData.error = error.message;
            statusData.errorTime = new Date().toISOString();
            
            await writeStatusFileAtomically(statusFilePath, statusData);
        } catch (updateError) {
            console.error(`‚ùå Failed to update error status:`, updateError.message);
        }
        
        return null;
    }
}

// ===== OLD COMPLEX VIDEO STRUCTURE LOGIC (COMMENTED OUT) =====
// OLD COMPLEX calculateVideoStructure function with tons of debugging and edge cases
// This caused audio cropping issues with complex duration calculations
/*
async function calculateVideoStructure_OLD_COMPLEX(audioDuration) {
    const introFolder = path.join(CONFIG.videoTemplatesBase, 'Intro');
    const loopFolder = path.join(CONFIG.videoTemplatesBase, 'Loop');
    const endFolder = path.join(CONFIG.videoTemplatesBase, 'End');
    
    // ... [old complex logic commented out] ...
    // This had tons of debugging, iteration tracking, duration validation
    // edge cases, safety bounds, etc. that were causing audio cropping
}
*/

/**
 * NEW SIMPLIFIED VIDEO STRUCTURE CALCULATION
 * Follow EXACT process:
 * 1. Get length of audio
 * 2. Insert one random @/Intro clip  
 * 3. Fill rest with random @/Loop clips until room for one more clip (leave space even if 0.0001 sec)
 * 4. Insert one random @/End clip
 * 5. Pad with 1 second of video at the end
 */
async function calculateVideoStructure(audioDuration) {
    console.log(`\nüéØ === NEW SIMPLIFIED VIDEO STRUCTURE ===`);
    console.log(`üéµ Audio duration: ${audioDuration.toFixed(3)}s`);
    
    const introFolder = path.join(CONFIG.videoTemplatesBase, 'Intro');
    const loopFolder = path.join(CONFIG.videoTemplatesBase, 'Loop');
    const endFolder = path.join(CONFIG.videoTemplatesBase, 'End');
    
    // STEP 1: Get one random intro clip
    const introFile = getRandomVideoFile(introFolder);
    if (!introFile) {
        throw new Error('Could not find intro video file');
    }
    const introDuration = await getMediaDuration(introFile);
    console.log(`üé¨ Selected intro: ${path.basename(introFile)} (${introDuration.toFixed(3)}s)`);
    
    // STEP 2: Get all loop files and their durations
    const loopFiles = [];
    if (fs.existsSync(loopFolder)) {
        const files = fs.readdirSync(loopFolder)
            .filter(file => CONFIG.supportedVideoFormats.some(ext => file.toLowerCase().endsWith(ext)))
            .filter(file => !file.startsWith('.'));
        
        for (const file of files) {
            const filePath = path.join(loopFolder, file);
            const duration = await getMediaDuration(filePath);
            loopFiles.push({ file: filePath, duration });
            console.log(`üîÑ Available loop: ${path.basename(filePath)} (${duration.toFixed(3)}s)`);
        }
    }
    
    if (loopFiles.length === 0) {
        throw new Error('Could not find any loop video files');
    }
    
    // STEP 3: Calculate target video duration (audio + padding)
    const targetVideoDuration = audioDuration + CONFIG.videoPaddingDuration;
    console.log(`üéØ Target video duration: ${targetVideoDuration.toFixed(3)}s (audio + ${CONFIG.videoPaddingDuration}s padding)`);
    
    // Calculate minimum time needed for loops
    const currentContentTime = introDuration;
    const timeNeededForLoops = targetVideoDuration - currentContentTime;
    console.log(`‚è±Ô∏è Current intro: ${currentContentTime.toFixed(3)}s`);
    console.log(`‚è±Ô∏è Time needed for loops: ${timeNeededForLoops.toFixed(3)}s`);
    
    // STEP 5: Fill with random loop clips to reach target duration
    const loops = [];
    let totalLoopTime = 0;
    let loopCount = 0;
    
    console.log(`\nüîÑ === FILLING WITH LOOP CLIPS TO REACH TARGET ===`);
    while (totalLoopTime < timeNeededForLoops) {
        // Get random loop file
        const randomLoop = loopFiles[Math.floor(Math.random() * loopFiles.length)];
        const remainingTimeNeeded = timeNeededForLoops - totalLoopTime;
        
        console.log(`üìä Loop ${loopCount + 1}:`);
        console.log(`   - Time still needed: ${remainingTimeNeeded.toFixed(3)}s`);
        console.log(`   - Random loop: ${path.basename(randomLoop.file)} (${randomLoop.duration.toFixed(3)}s)`);
        
        // Check if this loop will overfill the target
        if (randomLoop.duration <= remainingTimeNeeded) {
            // Loop fits completely - add full duration
            loops.push({
                file: randomLoop.file,
                duration: randomLoop.duration,
                fullDuration: true
            });
            totalLoopTime += randomLoop.duration;
            console.log(`   ‚úÖ Added full loop - total loop time now: ${totalLoopTime.toFixed(3)}s`);
        } else {
            // Loop would overfill - trim it to fit exactly
            loops.push({
                file: randomLoop.file,
                duration: remainingTimeNeeded,
                fullDuration: false
            });
            totalLoopTime += remainingTimeNeeded;
            console.log(`   ‚úÇÔ∏è Added trimmed loop (${remainingTimeNeeded.toFixed(3)}s of ${randomLoop.duration.toFixed(3)}s) - total loop time now: ${totalLoopTime.toFixed(3)}s`);
            
            // We've hit the exact target, break
            console.log(`   üéØ Exact target reached! Total loop time: ${totalLoopTime.toFixed(3)}s = needed: ${timeNeededForLoops.toFixed(3)}s`);
            break;
        }
        
        loopCount++;
    }
    
    console.log(`\nüìä === FINAL STRUCTURE ===`);
    console.log(`üé¨ Intro: ${path.basename(introFile)} (${introDuration.toFixed(3)}s)`);
    console.log(`üîÑ Loops: ${loops.length} clips (${totalLoopTime.toFixed(3)}s total)`);
    loops.forEach((loop, i) => {
        console.log(`   ${i+1}. ${path.basename(loop.file)} (${loop.duration.toFixed(3)}s)`);
    });
    
    const totalContentTime = introDuration + totalLoopTime;
    
    console.log(`‚è±Ô∏è Total content time: ${totalContentTime.toFixed(3)}s`);
    console.log(`üéØ Target was: ${targetVideoDuration.toFixed(3)}s`);
    console.log(`üéµ Audio time: ${audioDuration.toFixed(3)}s`);
    console.log(`üìä Video exceeds audio by: ${(totalContentTime - audioDuration).toFixed(3)}s`);
    
    // Verify we meet the minimum requirement
    if (totalContentTime < audioDuration) {
        console.warn(`üö® WARNING: Video (${totalContentTime.toFixed(3)}s) is shorter than audio (${audioDuration.toFixed(3)}s)!`);
        console.warn(`üö® This will cause audio cropping!`);
    } else {
        console.log(`‚úÖ Video is longer than audio - no audio cropping will occur`);
    }
    
    // ADD: Video structure debug summary before return
    console.log(`\nüîç === VIDEO STRUCTURE DEBUG SUMMARY ===`);
    console.log(`üìä Structure breakdown:`);
    console.log(`   - Intro: ${introDuration.toFixed(3)}s`);
    console.log(`   - Loops total: ${totalLoopTime.toFixed(3)}s (${loops.length} clips)`);
    console.log(`   - Content total: ${totalContentTime.toFixed(3)}s`);
    console.log(`üéµ Audio duration: ${audioDuration.toFixed(3)}s`);
    console.log(`‚öñÔ∏è Structure calculated total: ${totalContentTime.toFixed(3)}s`);
    console.log(`üîç Video vs audio diff: ${(totalContentTime - audioDuration).toFixed(3)}s`);
    if (totalContentTime >= audioDuration) {
        console.log(`‚úÖ SAFE: Video ‚â• Audio (no cropping)`);
    } else {
        console.log(`‚ùå DANGER: Video < Audio (will crop!)`);
    }
    console.log(`‚úÖ Structure calculation complete`);
    console.log(`=======================================\n`);
            
            return {
                intro: { file: introFile, duration: introDuration, fullDuration: true },
                loops: loops,
        totalVideoDuration: totalContentTime // Return actual content time, not target
    };
}

/**
 * Generate editly configuration for the video
 */
async function generateEditlyConfig(audioFile, videoStructure, outputPath, debugOverlay = false) {
    const { intro, loops } = videoStructure;
    
    // Get dimensions from intro video (all should match)
    const dimensions = await getVideoDimensions(intro.file);
    
    // Get actual audio duration to ensure exact matching
    const audioDuration = await getMediaDuration(audioFile);
    
    const clips = [];
    
    // Add intro clip (may be cropped for short audio)
    const introLayers = [{
        type: 'video',
        path: intro.file,
        // If intro video is cropped, specify the cutFrom
        ...(intro.fullDuration ? {} : { cutFrom: 0, cutTo: intro.duration })
    }];
    
    // Add debug overlay for intro if enabled
    if (debugOverlay) {
        const debugText = generateDebugText('intro', 1, await getMediaDuration(intro.file), intro.duration, 'none');
        introLayers.push({
            type: 'title',
            text: debugText,
            fontsize: 16,
            textColor: '#ffffff',
            position: { x: 0.02, y: 0.02, originX: 'left', originY: 'top' },
            box: 1,
            boxcolor: '#000000@0.7',
            boxborderw: 2
        });
    }
    
    clips.push({
        duration: intro.duration,
        layers: introLayers
        // NO transitions - completely removed
    });
    
    // Add loop clips (all full duration, NO TRANSITIONS for seamless looping)
    for (let i = 0; i < loops.length; i++) {
        const loop = loops[i];
        
        const loopLayers = [{
            type: 'video', 
            path: loop.file,
            // If loop is trimmed, specify the cutTo
            ...(loop.fullDuration ? {} : { cutFrom: 0, cutTo: loop.duration })
        }];
        
        // Add debug overlay for loop if enabled
        if (debugOverlay) {
            const debugText = generateDebugText('loop', i + 1, await getMediaDuration(loop.file), loop.duration, 'none');
            loopLayers.push({
                type: 'title',
                text: debugText,
                fontsize: 16,
                textColor: '#ffffff',
                position: { x: 0.02, y: 0.02, originX: 'left', originY: 'top' },
                box: 1,
                boxcolor: '#000000@0.7',
                boxborderw: 2
            });
        }
        
        clips.push({
            duration: loop.duration,
            layers: loopLayers
            // NO transition property = seamless cuts between loops
        });
    }
    
    // CRITICAL FIX: Calculate actual total clip duration instead of forcing audio + padding
    const totalClipDuration = clips.reduce((sum, clip) => sum + clip.duration, 0);
    
    const config = {
        outPath: outputPath,
        width: dimensions.width,
        height: dimensions.height,
        fps: 30,
        // FIXED: Use actual clip duration instead of forcing audio duration
        outDuration: totalClipDuration,
        audioFilePath: audioFile,
        keepSourceAudio: false, // Replace video audio with our audio
        // Use dummy transition with zero duration for seamless hard cuts
        defaults: {
            transition: { name: 'dummy', duration: 0 }
        },
        clips
    };

    // ENHANCED: Duration validation with comprehensive debugging  
    const expectedDuration = audioDuration + CONFIG.videoPaddingDuration;
    const configDurationDiff = Math.abs(totalClipDuration - expectedDuration);

    console.log(`\nüîç === EDITLY CONFIG DURATION VALIDATION ===`);
    console.log(`üéµ Audio duration: ${audioDuration.toFixed(3)}s`);
    console.log(`üìè Padding: ${CONFIG.videoPaddingDuration.toFixed(1)}s`);
    console.log(`üéØ Expected total (audio + padding): ${expectedDuration.toFixed(3)}s`);
    console.log(`üìä Clip breakdown:`);
    clips.forEach((clip, index) => {
        console.log(`   - Clip ${index + 1}: ${clip.duration.toFixed(3)}s`);
    });
    console.log(`üßÆ Total clip duration: ${totalClipDuration.toFixed(3)}s`);
    console.log(`‚öñÔ∏è Difference: ${configDurationDiff.toFixed(3)}s`);

    console.log(`\nüîß === DURATION STRATEGY ===`);
    if (configDurationDiff > 0.01) {
        console.log(`‚ö†Ô∏è MISMATCH DETECTED: Clips (${totalClipDuration.toFixed(3)}s) vs Expected (${expectedDuration.toFixed(3)}s)`);
        console.log(`üéØ SOLUTION: Using actual clip duration (${totalClipDuration.toFixed(3)}s) as outDuration`);
        console.log(`üìù Audio (${audioDuration.toFixed(3)}s) will naturally extend beyond video end`);
        console.log(`‚úÖ This prevents video stretching and maintains quality`);
    } else {
        console.log(`‚úÖ Duration validation passed - clips match expected duration`);
    }
    console.log(`üì§ Final outDuration: ${config.outDuration.toFixed(3)}s (matches total clips)`);
    console.log(`============================================\n`);
    
    return config;
}

/**
 * Post-process video: trim to audio + 1s, then append random End video
 * @param {string} inputVideoPath - Path to generated intro+loops video
 * @param {string} audioFilePath - Path to audio file for duration reference
 * @param {string} outputPath - Final output path
 * @param {boolean} debugMode - Enable debug logging
 * @returns {string} Path to final processed video
 */
async function postProcessVideoWithEnd(inputVideoPath, audioFilePath, outputPath, debugMode = false) {
    console.log(`\nüîß === POST-PROCESSING: TRIM + APPEND END ===`);
    
    // Get audio duration for exact trimming
    const audioDuration = await getMediaDuration(audioFilePath);
    const targetTrimDuration = audioDuration + CONFIG.videoPaddingDuration;
    
    console.log(`üéµ Audio duration: ${audioDuration.toFixed(3)}s`);
    console.log(`‚è∞ Target trim duration (audio + padding): ${targetTrimDuration.toFixed(3)}s`);
    
    // Get random End video
    const endFolder = path.join(CONFIG.videoTemplatesBase, 'End');
    const endVideoPath = getRandomVideoFile(endFolder);
    if (!endVideoPath) {
        throw new Error('No End video files found for appending');
    }
    const endDuration = await getMediaDuration(endVideoPath);
    console.log(`üéØ Selected end video: ${path.basename(endVideoPath)} (${endDuration.toFixed(3)}s)`);
    
    // Create intermediate trimmed video path
    const trimmedVideoPath = outputPath.replace('.mp4', '_trimmed.mp4');
    
    // Step 1: Trim input video to exact audio + 1s duration
    console.log(`\n‚úÇÔ∏è Step 1: Trimming video to ${targetTrimDuration.toFixed(3)}s`);
    const trimCommand = `ffmpeg -i "${inputVideoPath}" -t ${targetTrimDuration.toFixed(3)} -c copy "${trimmedVideoPath}"`;
    
    if (debugMode) {
        console.log(`üîç Trim command: ${trimCommand}`);
    }
    
    try {
        await execAsync(trimCommand, { cwd: CONFIG.outputFolder });
        
        // Verify trimmed video
        if (!fs.existsSync(trimmedVideoPath)) {
            throw new Error('Video trimming failed - no trimmed output created');
        }
        
        const actualTrimDuration = await getMediaDuration(trimmedVideoPath);
        console.log(`‚úÖ Video trimmed successfully: ${actualTrimDuration.toFixed(3)}s`);
        
        if (Math.abs(actualTrimDuration - targetTrimDuration) > 0.1) {
            console.warn(`‚ö†Ô∏è Trim duration mismatch: expected ${targetTrimDuration.toFixed(3)}s, got ${actualTrimDuration.toFixed(3)}s`);
        }
        
    } catch (trimError) {
        console.error(`‚ùå Video trimming failed:`, trimError.message);
        throw new Error(`Video trimming failed: ${trimError.message}`);
    }
    
    // Step 2: Create concatenation list for FFmpeg
    console.log(`\nüîó Step 2: Appending end video`);
    const concatListPath = path.join(CONFIG.outputFolder, `${path.basename(outputPath, '.mp4')}_concat_list.txt`);
    const concatContent = `file '${path.basename(trimmedVideoPath)}'\nfile '${path.relative(CONFIG.outputFolder, endVideoPath)}'`;
    fs.writeFileSync(concatListPath, concatContent);
    
    if (debugMode) {
        console.log(`üìã Concat list content:\n${concatContent}`);
    }
    
    // Step 3: Concatenate trimmed video + end video
    const concatCommand = `ffmpeg -f concat -safe 0 -i "${path.basename(concatListPath)}" -c copy "${path.basename(outputPath)}"`;
    
    if (debugMode) {
        console.log(`üîç Concat command: ${concatCommand}`);
    }
    
    try {
        await execAsync(concatCommand, { cwd: CONFIG.outputFolder });
        
        // Verify final video
        if (!fs.existsSync(outputPath)) {
            throw new Error('Video concatenation failed - no final output created');
        }
        
        const finalDuration = await getMediaDuration(outputPath);
        const expectedFinalDuration = targetTrimDuration + endDuration;
        
        console.log(`‚úÖ Final video created successfully`);
        console.log(`üìÅ Output: ${path.basename(outputPath)}`);
        console.log(`‚è±Ô∏è Final duration: ${finalDuration.toFixed(3)}s`);
        console.log(`üéØ Expected duration: ${expectedFinalDuration.toFixed(3)}s (${targetTrimDuration.toFixed(3)}s + ${endDuration.toFixed(3)}s)`);
        
        const finalDurationDiff = Math.abs(finalDuration - expectedFinalDuration);
        if (finalDurationDiff > 0.1) {
            console.warn(`‚ö†Ô∏è Final duration mismatch: ${finalDurationDiff.toFixed(3)}s difference`);
        } else {
            console.log(`‚úÖ Duration validation passed`);
        }
        
        // Cleanup intermediate files
        console.log(`\nüóëÔ∏è Cleaning up intermediate files`);
        if (fs.existsSync(trimmedVideoPath)) {
            fs.unlinkSync(trimmedVideoPath);
            console.log(`   - Deleted: ${path.basename(trimmedVideoPath)}`);
        }
        if (fs.existsSync(concatListPath)) {
            fs.unlinkSync(concatListPath);
            console.log(`   - Deleted: ${path.basename(concatListPath)}`);
        }
        
        console.log(`üéâ Post-processing complete!`);
        console.log(`üìä Structure: Intro+Loops(${targetTrimDuration.toFixed(3)}s) + End(${endDuration.toFixed(3)}s) = Total(${finalDuration.toFixed(3)}s)`);
        console.log(`===============================================\n`);
        
        return outputPath;
        
    } catch (concatError) {
        console.error(`‚ùå Video concatenation failed:`, concatError.message);
        
        // Cleanup on error
        if (fs.existsSync(trimmedVideoPath)) {
            fs.unlinkSync(trimmedVideoPath);
        }
        if (fs.existsSync(concatListPath)) {
            fs.unlinkSync(concatListPath);
        }
        
        throw new Error(`Video concatenation failed: ${concatError.message}`);
    }
}

async function generateSplitScreenVideo(statusData, statusFilePath, debugDuration = true) {
    try {
        console.log(`\nüé¨ === NEW SPLIT SCREEN VIDEO GENERATION ===`);
        
        // Extract required data from status
        const audioFile = path.resolve('../zero-wire/Spark-TTS', statusData.audioFile);
        let uploadedVideoPath = statusData.splitScreenPath;
        
        // If no uploaded video provided, select a random one from splitscreen folder
        if (!uploadedVideoPath || !fs.existsSync(uploadedVideoPath)) {
            console.log(`üìÅ No uploaded video - selecting random video from splitscreen folder`);
            uploadedVideoPath = getRandomVideoFile(CONFIG.splitscreenFolder);
            if (!uploadedVideoPath) {
                throw new Error('No videos found in splitscreen folder');
            }
            console.log(`üìπ Selected video: ${path.basename(uploadedVideoPath)}`);
        } else {
            console.log(`üìÅ Using uploaded video: ${path.basename(uploadedVideoPath)}`);
        }
        
        // Get uploaded video properties
        const uploadedVideoDuration = await getMediaDuration(uploadedVideoPath);
        const uploadedVideoDimensions = await getVideoDimensions(uploadedVideoPath);
        
        console.log(`‚è±Ô∏è Video duration: ${uploadedVideoDuration.toFixed(2)} seconds`);
        console.log(`üìê Video dimensions: ${uploadedVideoDimensions.width}x${uploadedVideoDimensions.height}`);
        
        // Generate output path with unique timestamp
        const audioBasename = path.basename(audioFile, path.extname(audioFile));
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-').substring(0, 19); // Include date and time
        const outputPath = path.join(CONFIG.outputFolder, `${audioBasename}_splitscreen_${timestamp}.mp4`);
        
        // Generate split screen editly config
        console.log(`üîß Generating NEW split screen configuration...`);
        const splitScreenConfig = await generateNewSplitScreenConfig(uploadedVideoPath, outputPath);
        
        // Save config file
        const configPath = path.join(CONFIG.outputFolder, `${audioBasename}_splitscreen_config.json5`);
        fs.writeFileSync(configPath, JSON.stringify(splitScreenConfig, null, 2));
        
        console.log(`üíæ Config saved: ${path.basename(configPath)}`);
        console.log(`üé¨ Executing split screen video generation...`);
        
        // Execute editly
        const editlyCommand = `${path.join(PROJECT_ROOT, 'node_modules/.bin/editly')} "${configPath}"`;
        await execAsync(editlyCommand);
        
        // Clean up config file
        if (fs.existsSync(configPath)) {
            fs.unlinkSync(configPath);
        }
        
        // Verify split screen output exists
        if (fs.existsSync(outputPath)) {
            const splitScreenDuration = await getMediaDuration(outputPath);
            console.log(`‚úÖ NEW Split screen video generated successfully`);
            console.log(`üìÅ Split screen output: ${path.basename(outputPath)}`);
            console.log(`‚è±Ô∏è Split screen duration: ${splitScreenDuration.toFixed(2)} seconds`);
            
            // === STEP 2: GENERATE NORMAL VIDEO ===
            console.log(`\nüé¨ === GENERATING NORMAL VIDEO FOR FUSION ===`);
            
            // Calculate video structure for normal video using TTS audio
            const audioDuration = await getMediaDuration(audioFile);
            const videoStructure = await calculateVideoStructure(audioDuration);
            
            // Generate normal video output path
            const normalVideoPath = path.join(CONFIG.outputFolder, `${audioBasename}_normal_${timestamp}.mp4`);
            
            console.log(`üéµ TTS audio duration: ${audioDuration.toFixed(2)} seconds`);
            // ADD: Enhanced TTS audio debugging
            console.log(`\nüîç === TTS AUDIO ANALYSIS ===`);
            console.log(`üìÅ Audio file: ${path.basename(audioFile)}`);
            console.log(`‚è±Ô∏è Raw audio duration: ${audioDuration.toFixed(3)} seconds`);
            console.log(`üéØ Target video duration: ${(audioDuration + CONFIG.videoPaddingDuration).toFixed(3)} seconds`);
            console.log(`üìè Padding: ${CONFIG.videoPaddingDuration.toFixed(1)} seconds`);
            console.log(`===============================\n`);
            console.log(`üìÅ Normal video output: ${path.basename(normalVideoPath)}`);
            
            // Generate normal video editly config
            const normalEditlyConfig = await generateEditlyConfig(audioFile, videoStructure, normalVideoPath, false);
            
            // Save normal video config
            const normalConfigPath = path.join(CONFIG.outputFolder, `${audioBasename}_normal_config.json5`);
            fs.writeFileSync(normalConfigPath, JSON.stringify(normalEditlyConfig, null, 2));
            
            console.log(`üîß Generating normal video configuration...`);
            console.log(`üíæ Normal config saved: ${path.basename(normalConfigPath)}`);
            
            // Execute editly for normal video
            const normalEditlyCommand = `${path.join(PROJECT_ROOT, 'node_modules/.bin/editly')} "${normalConfigPath}"`;
            console.log(`üé¨ Generating normal video with command: ${normalEditlyCommand}`);
            await execAsync(normalEditlyCommand);
            
            // Clean up normal config file
            if (fs.existsSync(normalConfigPath)) {
                fs.unlinkSync(normalConfigPath);
            }
            
            // Verify normal video was created
            if (!fs.existsSync(normalVideoPath)) {
                throw new Error('Normal video generation failed - no output file created');
            }
            
            const normalVideoDuration = await getMediaDuration(normalVideoPath);
            console.log(`‚úÖ Normal video generated successfully`);
            console.log(`üìÅ Normal video: ${path.basename(normalVideoPath)}`);
            console.log(`‚è±Ô∏è Normal video duration: ${normalVideoDuration.toFixed(2)} seconds`);
            
            // ADD: Normal video output verification
            console.log(`\nüîç === NORMAL VIDEO OUTPUT VERIFICATION ===`);
            console.log(`üéµ Expected duration (audio + padding): ${(audioDuration + CONFIG.videoPaddingDuration).toFixed(3)}s`);
            console.log(`üìπ Actual output duration: ${normalVideoDuration.toFixed(3)}s`);
            const normalDurationDiff = Math.abs(normalVideoDuration - (audioDuration + CONFIG.videoPaddingDuration));
            console.log(`‚öñÔ∏è Duration difference: ${normalDurationDiff.toFixed(3)}s`);
            if (normalDurationDiff > 0.1) {
                console.warn(`üö® NORMAL VIDEO DURATION MISMATCH! Expected ${(audioDuration + CONFIG.videoPaddingDuration).toFixed(3)}s, got ${normalVideoDuration.toFixed(3)}s`);
                console.warn(`‚ö†Ô∏è This may indicate audio cropping in the normal video generation!`);
            } else {
                console.log(`‚úÖ Normal video duration matches expected`);
            }
            console.log(`=============================================\n`);
            
            // === STEP 3: FUSE VIDEOS TOGETHER ===
            console.log(`\nüîó === FUSING SPLIT SCREEN + NORMAL VIDEO ===`);
            
            // Generate final fused output path
            const fusedVideoPath = path.join(CONFIG.outputFolder, `${audioBasename}_fused_splitscreen_${timestamp}.mp4`);
            
            // Create concat file list for ffmpeg
            const concatListPath = path.join(CONFIG.outputFolder, `${audioBasename}_concat_list.txt`);
            const concatContent = `file '${path.basename(outputPath)}'\nfile '${path.basename(normalVideoPath)}'`;
            fs.writeFileSync(concatListPath, concatContent);
            
            console.log(`üìã Concat list created: ${path.basename(concatListPath)}`);
            console.log(`üé¨ Part 1: Split screen video (${splitScreenDuration.toFixed(2)}s)`);
            console.log(`üé¨ Part 2: Normal video (${normalVideoDuration.toFixed(2)}s)`);
            console.log(`üìÅ Final output: ${path.basename(fusedVideoPath)}`);
            
            // Execute ffmpeg concat (use relative paths since we're running from CONFIG.outputFolder)
            const ffmpegConcatCommand = `ffmpeg -f concat -safe 0 -i "${path.basename(concatListPath)}" -c copy "${path.basename(fusedVideoPath)}"`;
            console.log(`üîó Fusing videos with command: ${ffmpegConcatCommand}`);
            
            try {
                await execAsync(ffmpegConcatCommand, { cwd: CONFIG.outputFolder });
                
                // Verify fused video was created
                if (fs.existsSync(fusedVideoPath)) {
                    const fusedVideoDuration = await getMediaDuration(fusedVideoPath);
                    const expectedDuration = splitScreenDuration + normalVideoDuration;
                    
                    console.log(`‚úÖ FUSED VIDEO GENERATED SUCCESSFULLY!`);
                    console.log(`üìÅ Final output: ${path.basename(fusedVideoPath)}`);
                    console.log(`‚è±Ô∏è Total duration: ${fusedVideoDuration.toFixed(2)}s (expected: ${expectedDuration.toFixed(2)}s)`);
                    
                    // ADD: Final fusion verification
                    console.log(`\nüîç === FINAL FUSION VERIFICATION ===`);
                    console.log(`üé¨ Split screen duration: ${splitScreenDuration.toFixed(3)}s`);
                    console.log(`üìπ Normal video duration: ${normalVideoDuration.toFixed(3)}s`);
                    console.log(`üßÆ Expected fused duration: ${expectedDuration.toFixed(3)}s`);
                    console.log(`üé• Actual fused duration: ${fusedVideoDuration.toFixed(3)}s`);
                    const fusionDiff = Math.abs(fusedVideoDuration - expectedDuration);
                    console.log(`‚öñÔ∏è Fusion difference: ${fusionDiff.toFixed(3)}s`);
                    console.log(`üéµ Original audio + padding: ${(audioDuration + CONFIG.videoPaddingDuration).toFixed(3)}s`);
                    const totalAudioDiff = Math.abs(fusedVideoDuration - (splitScreenDuration + audioDuration + CONFIG.videoPaddingDuration));
                    console.log(`üîç Total vs audio difference: ${totalAudioDiff.toFixed(3)}s`);
                    if (fusionDiff > 0.1) {
                        console.warn(`üö® FUSION DURATION MISMATCH! Parts don't add up correctly.`);
                    } else {
                        console.log(`‚úÖ Fusion duration matches expected sum of parts`);
                    }
                    if (totalAudioDiff > 0.1) {
                        console.warn(`üö® AUDIO SYNC ISSUE! Final video doesn't match expected audio timing.`);
                    } else {
                        console.log(`‚úÖ Final video duration correctly synced with audio`);
                    }
                    console.log(`=====================================\n`);
                    
                    // Clean up intermediate files
                    console.log(`üóëÔ∏è Cleaning up intermediate files...`);
                    if (fs.existsSync(concatListPath)) {
                        fs.unlinkSync(concatListPath);
                        console.log(`   - Deleted: ${path.basename(concatListPath)}`);
                    }
                    if (fs.existsSync(outputPath)) {
                        fs.unlinkSync(outputPath);
                        console.log(`   - Deleted: ${path.basename(outputPath)} (split screen only)`);
                    }
                    if (fs.existsSync(normalVideoPath)) {
                        fs.unlinkSync(normalVideoPath);
                        console.log(`   - Deleted: ${path.basename(normalVideoPath)} (normal video only)`);
                    }
                    
                    console.log(`üéâ FINAL FUSED SPLIT SCREEN VIDEO COMPLETE!`);
                    return fusedVideoPath;
                } else {
                    throw new Error('Video fusion failed - no fused output file created');
                }
                
            } catch (ffmpegError) {
                console.error(`‚ùå FFmpeg fusion failed:`, ffmpegError.message);
                // Clean up concat list file
                if (fs.existsSync(concatListPath)) {
                    fs.unlinkSync(concatListPath);
                }
                throw new Error(`Video fusion failed: ${ffmpegError.message}`);
            }
            
        } else {
            throw new Error('Split screen video generation failed - no output file created');
        }
        
    } catch (error) {
        console.error(`‚ùå NEW Split screen video generation failed:`, error.message);
        return null;
    }
}

/**
 * NEW Split Screen Configuration Generator
 * Creates proper cropping with center positioning according to requirements
 */
async function generateNewSplitScreenConfig(uploadedVideoPath, outputPath) {
    console.log(`\nüîß === NEW SPLIT SCREEN CONFIG GENERATION ===`);
    console.log(`üìÅ Uploaded video: ${path.basename(uploadedVideoPath)}`);
    console.log(`üì§ Output path: ${path.basename(outputPath)}`);
    
    // Get random template video from splitscreen folder
    const templateVideoPath = getRandomVideoFile(CONFIG.splitscreenFolder);
    if (!templateVideoPath) {
        throw new Error('No template video found in splitscreen folder');
    }
    console.log(`üìπ Template video: ${path.basename(templateVideoPath)}`);
    
    // Get video properties
    const uploadedVideoDuration = await getMediaDuration(uploadedVideoPath);
    const uploadedVideoDimensions = await getVideoDimensions(uploadedVideoPath);
    const templateVideoDimensions = await getVideoDimensions(templateVideoPath);
    
    console.log(`‚è±Ô∏è Duration: ${uploadedVideoDuration.toFixed(2)}s (from uploaded video)`);
    console.log(`üìê Canvas: ${uploadedVideoDimensions.width}x${uploadedVideoDimensions.height} (from uploaded video)`);
    console.log(`üìê Template: ${templateVideoDimensions.width}x${templateVideoDimensions.height}`);
    
    // NEW SPLIT SCREEN LAYER CONFIGURATION
    const splitScreenLayers = [
        // TOP VIDEO: Template video - crop center 50%, fill top half of screen
        {
            type: 'video',
            path: templateVideoPath,
            resizeMode: 'cover',      // Changed from 'crop' to 'cover'
            left: 0,                  // X-position: left edge
            top: 0,                   // Y-position: top of screen
            width: 1.0,               // Full canvas width
            height: 0.5,              // 50% of canvas height (top half)
            cutFrom: 0,               // Start from beginning
            cutTo: uploadedVideoDuration,  // Match uploaded video duration
            originX: 'left',
            originY: 'top'
        },
        // BOTTOM VIDEO: Uploaded video - crop center 50%, fill bottom half of screen
        {
            type: 'video',
            path: uploadedVideoPath,
            resizeMode: 'cover',      // Changed from 'crop' to 'cover'
            left: 0,                  // X-position: left edge
            top: 0.5,                 // Y-position: 50% down (bottom half)
            width: 1.0,               // Full canvas width
            height: 0.5,              // 50% of canvas height (bottom half)
            cutFrom: 0,               // Start from beginning
            cutTo: uploadedVideoDuration,  // Full duration
            originX: 'left',
            originY: 'top'
        }
    ];
    
    console.log(`\nüéØ === NEW SPLIT SCREEN LAYER DETAILS ===`);
    console.log(`üìπ TOP LAYER (Template - top 50% of screen):`);
    console.log(`   - File: ${path.basename(templateVideoPath)}`);
    console.log(`   - Canvas Position: left=${splitScreenLayers[0].left}, top=${splitScreenLayers[0].top} (top half)`);
    console.log(`   - Canvas Size: width=${splitScreenLayers[0].width}, height=${splitScreenLayers[0].height} (50% height)`);
    console.log(`   - Duration: ${splitScreenLayers[0].cutFrom}s to ${splitScreenLayers[0].cutTo}s`);
    console.log(`   - Resize Mode: ${splitScreenLayers[0].resizeMode}`);
    
    console.log(`üìπ BOTTOM LAYER (Uploaded - bottom 50% of screen):`);
    console.log(`   - File: ${path.basename(uploadedVideoPath)}`);
    console.log(`   - Canvas Position: left=${splitScreenLayers[1].left}, top=${splitScreenLayers[1].top} (bottom half)`);
    console.log(`   - Canvas Size: width=${splitScreenLayers[1].width}, height=${splitScreenLayers[1].height} (50% height)`);
    console.log(`   - Duration: ${splitScreenLayers[1].cutFrom}s to ${splitScreenLayers[1].cutTo}s`);
    console.log(`   - Resize Mode: ${splitScreenLayers[1].resizeMode}`);
    
    // Create single clip with both layers
    const clips = [{
        duration: uploadedVideoDuration + CONFIG.videoPaddingDuration,
        layers: splitScreenLayers
    }];
    
    // Build final editly configuration
    const config = {
        outPath: outputPath,
        width: uploadedVideoDimensions.width,
        height: uploadedVideoDimensions.height,
        fps: 30,
        outDuration: uploadedVideoDuration + CONFIG.videoPaddingDuration,
        keepSourceAudio: true,  // FIXED: Keep source audio so uploaded video audio is preserved
        defaults: {
            transition: { name: 'dummy', duration: 0 }
        },
        clips
    };
    
    // Set audio to come from uploaded video
    if (clips.length > 0) {
        clips[0].audioFilePath = uploadedVideoPath;
        console.log(`üîä Audio source: ${path.basename(uploadedVideoPath)} (uploaded video audio - keepSourceAudio: true)`);
    }
    
    console.log(`\nüìä === NEW SPLIT SCREEN SUMMARY ===`);
    console.log(`üé¨ Total clips: ${clips.length}`);
    console.log(`‚è±Ô∏è Audio duration: ${uploadedVideoDuration.toFixed(2)}s`);
    console.log(`‚è±Ô∏è Video duration (with padding): ${(uploadedVideoDuration + CONFIG.videoPaddingDuration).toFixed(2)}s`);
    console.log(`üéØ Padding: ${CONFIG.videoPaddingDuration.toFixed(1)}s added to prevent audio cutoff`);
    console.log(`üìê Resolution: ${uploadedVideoDimensions.width}x${uploadedVideoDimensions.height}`);
    console.log(`üéûÔ∏è Frame rate: 30fps`);
    console.log(`üéµ Audio: From uploaded video`);
    console.log(`üì§ Output: ${path.basename(outputPath)}`);
    console.log(`===============================================\n`);
    
    return config;
}

/**
 * Generate editly configuration for split-screen + normal video combination
 */
async function generateSplitScreenEditlyConfig(audioFilePath, splitscreenDuration, splitscreenVideoPath, normalVideoPath, outputPath, debugOverlay = false, splitscreenSourcePath = null) {
    console.log(`\nüîß === SPLIT-SCREEN + NORMAL VIDEO CONFIG GENERATION ===`);
    
    // Get audio duration for padding calculation
    const audioDuration = await getMediaDuration(audioFilePath);
    const normalVideoDuration = await getMediaDuration(normalVideoPath);
    
    // Get dimensions from normal video
    const dimensions = await getVideoDimensions(normalVideoPath);
    
    const clips = [];
    
    // Clip 1: Split-screen segment (uses source audio from splitscreen video)
    const splitscreenLayers = [{
        type: 'video',
        path: splitscreenVideoPath
    }];
    
    if (debugOverlay) {
        splitscreenLayers.push({
            type: 'title',
            text: `SPLITSCREEN | Duration: ${splitscreenDuration.toFixed(2)}s | Source Audio`,
            fontsize: 16,
            textColor: '#ffffff',
            position: { x: 0.02, y: 0.02, originX: 'left', originY: 'top' },
            box: 1,
            boxcolor: '#000000@0.7',
            boxborderw: 2
        });
    }
    
    clips.push({
        duration: splitscreenDuration,
        layers: splitscreenLayers,
        audioFilePath: splitscreenSourcePath || splitscreenVideoPath  // Use source audio
    });
    
    // Clip 2: Normal video segment (uses TTS audio) - includes padding
    const normalLayers = [{
        type: 'video',
        path: normalVideoPath
    }];
    
    if (debugOverlay) {
        normalLayers.push({
            type: 'title',
            text: `NORMAL VIDEO | Duration: ${normalVideoDuration.toFixed(2)}s | TTS Audio + ${CONFIG.videoPaddingDuration.toFixed(1)}s Padding`,
            fontsize: 16,
            textColor: '#ffffff',
            position: { x: 0.02, y: 0.02, originX: 'left', originY: 'top' },
            box: 1,
            boxcolor: '#000000@0.7',
            boxborderw: 2
        });
    }
    
    clips.push({
        duration: normalVideoDuration,  // This already includes padding from generateEditlyConfig
        layers: normalLayers,
        audioFilePath: audioFilePath  // Use TTS audio
    });
    
    const totalDuration = splitscreenDuration + normalVideoDuration;
    
    const config = {
        outPath: outputPath,
        width: dimensions.width,
        height: dimensions.height,
        fps: 30,
        outDuration: totalDuration,  // Total includes padding in normal video segment
        keepSourceAudio: false,  // We're managing audio per clip
        defaults: {
            transition: { name: 'dummy', duration: 0 }
        },
        clips
    };
    
    console.log(`üìä Split-screen config summary:`);
    console.log(`   üîÄ Split-screen segment: ${splitscreenDuration.toFixed(2)}s (with source audio)`);
    console.log(`   üé¨ Normal video segment: ${normalVideoDuration.toFixed(2)}s (with TTS audio + ${CONFIG.videoPaddingDuration.toFixed(1)}s padding)`);
    console.log(`   üì∫ Total duration: ${totalDuration.toFixed(2)}s`);
    
    return config;
}

/**
 * Process new audio file
 */
async function processAudioFile(audioFilePath, debugOverlay = false, splitScreenMode = false, splitScreenClipPath = null) {
    // FIXED: Use mode-specific lock files to prevent race conditions between normal and split screen processing
    const processingMode = splitScreenMode ? 'splitscreen' : 'normal';
    const lockFile = audioFilePath + `.processing_${processingMode}`;
    
    try {
        // Check for processing lock to prevent duplicate processing in SAME MODE
        if (fs.existsSync(lockFile)) {
            console.log(`üîí File already being processed in ${processingMode} mode: ${path.basename(audioFilePath)}`);
            // Wait for the other process to complete and return its result
            const maxWait = 60000; // 60 seconds
            const checkInterval = 1000; // 1 second
            let waited = 0;
            
            while (waited < maxWait) {
                if (!fs.existsSync(lockFile)) {
                    // Other process completed, check if video exists for THIS MODE
                    const audioBasename = path.basename(audioFilePath, path.extname(audioFilePath));
                    const timestamp = new Date().toISOString().split('T')[0];
                    
                    let expectedOutputPath;
                    if (splitScreenMode) {
                        expectedOutputPath = path.join(CONFIG.finalOutputFolder, `${audioBasename}_splitscreen_${timestamp}.mp4`);
                    } else {
                        expectedOutputPath = path.join(CONFIG.outputFolder, `${audioBasename}_${timestamp}.mp4`);
                    }
                    
                    if (fs.existsSync(expectedOutputPath)) {
                        console.log(`‚úÖ Found ${processingMode} video created by parallel process: ${path.basename(expectedOutputPath)}`);
                        return expectedOutputPath;
                    }
                    break;
                }
                await new Promise(resolve => setTimeout(resolve, checkInterval));
                waited += checkInterval;
            }
            
            console.log(`‚è∞ Timeout waiting for parallel ${processingMode} process`);
            return null;
        }
        
        // Create mode-specific processing lock
        fs.writeFileSync(lockFile, `${Date.now()}_${processingMode}`);
        
        console.log(`\nüéµ Processing new audio file: ${path.basename(audioFilePath)}`);
        
        if (splitScreenMode) {
            console.log('üîÄ Split-screen mode enabled');
        }
        
        // Check if file still exists (may have been processed by another instance)
        if (!fs.existsSync(audioFilePath)) {
            console.warn(`‚ö†Ô∏è Audio file no longer exists, skipping: ${path.basename(audioFilePath)}`);
            // Clean up lock file
            if (fs.existsSync(lockFile)) {
                fs.unlinkSync(lockFile);
            }
            return null;
        }
        
        // Get audio duration
        const audioDuration = await getMediaDuration(audioFilePath);
        console.log(`‚è±Ô∏è Audio duration: ${audioDuration.toFixed(2)} seconds`);
        
        if (audioDuration <= 0) {
            console.error('‚ùå Invalid audio duration');
            // Clean up lock file
            if (fs.existsSync(lockFile)) {
                fs.unlinkSync(lockFile);
            }
            return null;
        }
        
        // Generate base filename components with unique timestamp
        const audioBasename = path.basename(audioFilePath, path.extname(audioFilePath));
        const timestamp = new Date().toISOString().replace(/[:.]/g, '-').substring(0, 19); // Include date and time
        
        if (splitScreenMode) {
            // SPLIT-SCREEN MODE: Generate normal video first, then combine with split-screen
            console.log('üé¨ Generating normal video structure for combination...');
            
            // Step 1: Generate normal video
            const videoStructure = await calculateVideoStructure(audioDuration);
            const tempOutputPath = path.join(CONFIG.outputFolder, `temp_normal_${audioBasename}_${timestamp}.mp4`);
            
            console.log('‚öôÔ∏è Generating normal video configuration...');
            const normalEditlyConfig = await generateEditlyConfig(audioFilePath, videoStructure, tempOutputPath, debugOverlay);
            
            const normalConfigPath = path.join(CONFIG.outputFolder, `temp_normal_${audioBasename}_config.json5`);
            fs.writeFileSync(normalConfigPath, JSON.stringify(normalEditlyConfig, null, 2));
            
            console.log('üé• Generating normal video...');
            const editlyCommand = `${path.join(PROJECT_ROOT, 'node_modules/.bin/editly')} "${normalConfigPath}"`;
            
            console.log(`üé¨ Generating normal video with command: ${editlyCommand}`);
            await execAsync(editlyCommand);
            
            // Clean up normal config
            if (fs.existsSync(normalConfigPath)) {
                fs.unlinkSync(normalConfigPath);
            }
            
            console.log('‚úÖ Normal video generated');
            
            // Add duration validation and logging
            const actualNormalVideoDuration = await getMediaDuration(tempOutputPath);
            console.log(`üîç Duration validation - Audio: ${audioDuration.toFixed(2)}s, Expected video (with padding): ${(audioDuration + CONFIG.videoPaddingDuration).toFixed(2)}s, Actual video: ${actualNormalVideoDuration.toFixed(2)}s`);
            
            // Check for duration mismatch and regenerate if needed
            const expectedVideoDuration = audioDuration + CONFIG.videoPaddingDuration;
            const durationDifference = Math.abs(actualNormalVideoDuration - expectedVideoDuration);
            if (durationDifference > 0.1) {
                console.warn(`‚ö†Ô∏è Duration mismatch detected! Regenerating normal video with forced duration...`);
                console.log(`   üìä Difference: ${durationDifference.toFixed(2)}s (tolerance: 0.1s)`);
                
                // Force video structure to match exact audio duration
                const forcedVideoStructure = await calculateVideoStructure(audioDuration);
                
                // Regenerate editly config with forced duration
                const forcedEditlyConfig = await generateEditlyConfig(audioFilePath, forcedVideoStructure, tempOutputPath, debugOverlay);
                
                // Save and execute forced config
                const forcedConfigPath = path.join(CONFIG.outputFolder, `temp_forced_${audioBasename}_config.json5`);
                fs.writeFileSync(forcedConfigPath, JSON.stringify(forcedEditlyConfig, null, 2));
                
                console.log('üîÑ Regenerating normal video with exact audio duration...');
                const forcedEditlyCommand = `${path.join(PROJECT_ROOT, 'node_modules/.bin/editly')} "${forcedConfigPath}"`;
                await execAsync(forcedEditlyCommand);
                
                // Clean up forced config
                if (fs.existsSync(forcedConfigPath)) {
                    fs.unlinkSync(forcedConfigPath);
                }
                
                // Verify the fix worked
                const verifyDuration = await getMediaDuration(tempOutputPath);
                console.log(`‚úÖ Normal video regenerated - New duration: ${verifyDuration.toFixed(2)}s`);
                
                if (Math.abs(verifyDuration - audioDuration) > 0.1) {
                    console.warn(`‚ö†Ô∏è Duration still mismatched after regeneration: ${verifyDuration.toFixed(2)}s vs ${audioDuration.toFixed(2)}s`);
                }
            } else {
                console.log(`‚úÖ Duration validation passed - Difference: ${durationDifference.toFixed(3)}s`);
            }
            
            // Step 2: Generate split-screen clip
            console.log('üîÄ Preparing split-screen generation...');
            
            // Get videos for split-screen generation
            const splitscreenTopFile = getRandomVideoFile(CONFIG.splitscreenFolder);
            let splitscreenBottomFile;
            
            if (!splitscreenTopFile) {
                throw new Error(`No videos found in splitscreen folder: ${CONFIG.splitscreenFolder}`);
            }
            
            // Use uploaded file if provided, otherwise fallback to random selection
            if (splitScreenClipPath && fs.existsSync(splitScreenClipPath)) {
                console.log(`üé¨ Using uploaded split-screen clip: ${path.basename(splitScreenClipPath)}`);
                splitscreenBottomFile = splitScreenClipPath;
                
                // Validate that the uploaded file is a video
                const ext = path.extname(splitScreenClipPath).toLowerCase();
                if (!CONFIG.supportedVideoFormats.includes(ext)) {
                    throw new Error(`Uploaded file is not a supported video format: ${ext}`);
                }
            } else {
                if (splitScreenClipPath) {
                    console.warn(`‚ö†Ô∏è Uploaded split-screen clip not found: ${splitScreenClipPath}, falling back to random selection`);
                }
                
                splitscreenBottomFile = getRandomVideoFile(CONFIG.splitscreenSourceFolder);
                
                if (!splitscreenBottomFile) {
                    console.warn('‚ö†Ô∏è No videos found in splitscreensource folder, using splitscreen video for both top and bottom');
                    // Use the same video for both if splitscreensource is empty
                    splitscreenBottomFile = splitscreenTopFile;
                }
            }
            
            // Define split-screen duration based on splitscreensource video
            let splitscreenDuration;
            try {
                splitscreenDuration = await getMediaDuration(splitscreenBottomFile);
                console.log(`üîÄ Using splitscreensource duration: ${splitscreenDuration.toFixed(2)}s`);
            } catch (error) {
                console.warn('‚ö†Ô∏è Could not get splitscreensource duration, falling back to 4 seconds:', error.message);
                splitscreenDuration = 4.0;
            }
            
            const splitscreenOutputPath = path.join(CONFIG.outputFolder, `temp_splitscreen_${audioBasename}_${timestamp}.mp4`);
            
            // Generate split-screen video
            await generateSplitScreenClip(splitscreenTopFile, splitscreenBottomFile, splitscreenOutputPath, splitscreenDuration);
            
            /* OLD BUGGY APPROACH - COMMENTED OUT TO PREVENT FUTURE CONFUSION
             * This approach concatenated splitscreensource audio + Spark TTS audio, 
             * which caused audio to stop halfway through and sync issues.
             * The correct approach is to use per-clip audio (silent split screen + audio on normal video).
             */
            
            // Step 3: CORRECTED APPROACH - Use per-clip audio for proper synchronization
            console.log('üîó Using per-clip audio approach (splitscreen source audio + Spark TTS on normal video)...');
            
            // Get final normal video duration for logging
            const finalNormalVideoDuration = await getMediaDuration(tempOutputPath);
            
            // Comprehensive duration logging for debugging
            console.log(`üìä Final duration summary:`);
            console.log(`   üîÄ Split-screen duration: ${splitscreenDuration.toFixed(2)}s (with source audio)`);
            console.log(`   üéµ Spark TTS audio duration: ${audioDuration.toFixed(2)}s`);
            console.log(`   üé¨ Normal video duration: ${finalNormalVideoDuration.toFixed(2)}s (includes ${CONFIG.videoPaddingDuration.toFixed(1)}s padding)`);
            console.log(`   üéØ Padding ensures audio completion before video ends`);
            console.log(`   üì∫ Total video will be: ${(splitscreenDuration + finalNormalVideoDuration).toFixed(2)}s`);
            console.log(`   üîä Audio sync: Split-screen plays source audio, then Spark TTS starts with normal video (${splitscreenDuration.toFixed(2)}s offset)`);
            
            /* REMOVED: Audio extraction, concatenation, and validation
             * - No splitscreenAudioPath needed
             * - No combinedAudioPath needed  
             * - No combineAudioCommand needed
             * - No audio duration validation needed
             * This eliminates the source of audio sync bugs.
             */
            
            // Step 4: Combine split-screen + normal video using Editly (with per-clip audio)
            console.log('üîó Combining split-screen and normal video with per-clip audio synchronization...');
            
            const finalOutputPath = path.join(CONFIG.finalOutputFolder, `${audioBasename}_splitscreen_${timestamp}.mp4`);
            
            // Ensure final output directory exists
            if (!fs.existsSync(CONFIG.finalOutputFolder)) {
                fs.mkdirSync(CONFIG.finalOutputFolder, { recursive: true });
            }
            
            // Generate split-screen + normal video configuration (with per-clip audio)
            console.log('‚öôÔ∏è Generating split-screen + normal video configuration...');
            const splitscreenConfig = await generateSplitScreenEditlyConfig(audioFilePath, splitscreenDuration, splitscreenOutputPath, tempOutputPath, finalOutputPath, debugOverlay, splitscreenBottomFile);
            
            const splitscreenConfigPath = path.join(CONFIG.outputFolder, `temp_splitscreen_${audioBasename}_config.json5`);
            fs.writeFileSync(splitscreenConfigPath, JSON.stringify(splitscreenConfig, null, 2));
            
            console.log('üé• Generating final split-screen video with synchronized audio...');
            const finalEditlyCommand = `${path.join(PROJECT_ROOT, 'node_modules/.bin/editly')} "${splitscreenConfigPath}"`;
            
            console.log(`üé¨ Generating final video with command: ${finalEditlyCommand}`);
            await execAsync(finalEditlyCommand);
            
            // Clean up temporary files
            [tempOutputPath, splitscreenOutputPath, splitscreenConfigPath].forEach(file => {
                if (fs.existsSync(file)) {
                    fs.unlinkSync(file);
                    console.log(`üóëÔ∏è Cleaned up: ${path.basename(file)}`);
                }
            });
            
            console.log('‚úÖ Split-screen video generation complete!');
            console.log(`üìÅ Final output: ${finalOutputPath}`);
            console.log(`üì¶ Split-screen intro (${splitscreenDuration.toFixed(2)}s with source audio) + Normal video (${finalNormalVideoDuration.toFixed(2)}s with Spark TTS audio + ${CONFIG.videoPaddingDuration.toFixed(1)}s padding)`);
            console.log(`üîä Total video duration: ${(splitscreenDuration + finalNormalVideoDuration).toFixed(2)}s with properly synchronized audio and padding to prevent cutoff`);
            
            // Clean up lock file
            if (fs.existsSync(lockFile)) {
                fs.unlinkSync(lockFile);
            }
            
            return finalOutputPath;
            
        } else {
            // NORMAL MODE: Standard video generation
        console.log('üé¨ Calculating video structure...');
        const videoStructure = await calculateVideoStructure(audioDuration);
        
        const outputPath = path.join(CONFIG.outputFolder, `${audioBasename}_${timestamp}.mp4`);
        
        // Check if video already exists
        if (fs.existsSync(outputPath)) {
                console.warn(`‚ö†Ô∏è Video already exists: ${path.basename(outputPath)}`);
                console.log(`‚úÖ Using existing video file`);
                // Clean up lock file
                if (fs.existsSync(lockFile)) {
                    fs.unlinkSync(lockFile);
                }
                return outputPath;
        }
        
        // Generate editly config
        console.log('‚öôÔ∏è Generating video configuration...');
        const editlyConfig = await generateEditlyConfig(audioFilePath, videoStructure, outputPath, debugOverlay);
        
        // Save config file for debugging
        const configPath = path.join(CONFIG.outputFolder, `${audioBasename}_config.json5`);
        fs.writeFileSync(configPath, JSON.stringify(editlyConfig, null, 2));
        console.log(`üíæ Config saved: ${configPath}`);
        
        // Double-check audio file still exists before video generation
        if (!fs.existsSync(audioFilePath)) {
            console.warn(`‚ö†Ô∏è Audio file disappeared during processing: ${path.basename(audioFilePath)}`);
            // Clean up config file
            if (fs.existsSync(configPath)) {
                fs.unlinkSync(configPath);
            }
                // Clean up lock file
                if (fs.existsSync(lockFile)) {
                    fs.unlinkSync(lockFile);
                }
                return null;
        }
        
        // Generate video using editly
        console.log('üé• Generating video...');
            const editlyCommand = `./node_modules/.bin/editly "${configPath}"`;
        
            console.log(`üé¨ Generating video with command: ${editlyCommand}`);
        const { stdout, stderr } = await execAsync(editlyCommand);
        
        if (stderr && !stderr.includes('ffmpeg version')) {
            console.warn('‚ö†Ô∏è Editly warnings:', stderr);
        }
        
        // üîç CRITICAL FIX: Verify file was actually created before claiming success
        if (!fs.existsSync(outputPath)) {
            throw new Error(`Video generation failed: Output file not created at ${outputPath}. Editly command appeared to succeed but no file was generated.`);
        }
        
        // POST-PROCESSING: Trim video and append End
        console.log('\nüîß Starting post-processing: trim + append end...');
        const tempOutputPath = outputPath.replace('.mp4', '_temp.mp4');
        
        // Rename current output to temp
        fs.renameSync(outputPath, tempOutputPath);
        
        // Post-process: trim + append end
        const finalOutputPath = await postProcessVideoWithEnd(tempOutputPath, audioFilePath, outputPath, debugOverlay);
        
        // Clean up temp file
        if (fs.existsSync(tempOutputPath)) {
            fs.unlinkSync(tempOutputPath);
            console.log(`üóëÔ∏è Cleaned up temp file: ${path.basename(tempOutputPath)}`);
        }
        
        // Update video structure reference for logging
        const finalDuration = await getMediaDuration(finalOutputPath);
        console.log(`‚úÖ Post-processing complete! Final duration: ${finalDuration.toFixed(2)}s`);
        
        console.log('‚úÖ Video generation complete!');
        console.log(`üìÅ Output: ${outputPath}`);
        console.log(`üìä Final video: ${finalDuration.toFixed(2)}s`);
        
        // Clean up config file
        if (fs.existsSync(configPath)) {
            fs.unlinkSync(configPath);
        }
        
        console.log('‚ú® Audio processing pipeline complete!');
        console.log(`üì¶ Audio file remains in done folder: ${path.basename(audioFilePath)}`);
            
            // Clean up lock file
            if (fs.existsSync(lockFile)) {
                fs.unlinkSync(lockFile);
            }
        
        return outputPath;
        }
        
    } catch (error) {
        console.error('‚ùå Error processing audio file:', error.message);
        console.error(error.stack);
        
        // Clean up any partial config files on error
        try {
            const audioBasename = path.basename(audioFilePath, path.extname(audioFilePath));
            const configPath = path.join(CONFIG.outputFolder, `${audioBasename}_config.json5`);
            if (fs.existsSync(configPath)) {
                fs.unlinkSync(configPath);
                console.log('üóëÔ∏è Cleaned up config file after error');
            }
        } catch (cleanupError) {
            console.error('‚ùå Error during cleanup:', cleanupError.message);
        }
        
        // Clean up lock file
        if (fs.existsSync(lockFile)) {
            fs.unlinkSync(lockFile);
        }
        
        // Return null to indicate failure
        return null;
    }
}

/**
 * Initialize file watcher
 */
function initializeWatcher(debugOverlay = false, splitScreenMode = false) {
    const watchPath = path.resolve(CONFIG.audioWatchFolder);
    const statusPath = path.resolve('./status');
    
    if (!fs.existsSync(watchPath)) {
        console.error(`‚ùå Audio watch folder not found: ${watchPath}`);
        console.log('üí° Please ensure the Spark-TTS audiooutput/done folder exists');
        console.log('üí° The done folder is created automatically when audio files are generated');
        process.exit(1);
    }
    
    // Ensure status directory exists
    if (!fs.existsSync(statusPath)) {
        fs.mkdirSync(statusPath, { recursive: true });
    }
    
    console.log(`üëÄ Watching for new audio files in: ${watchPath}`);
    console.log(`üìÅ Video templates: ${path.resolve(CONFIG.videoTemplatesBase)}`);
    console.log(`üîÑ Status coordination: ${statusPath}`);
    
    if (splitScreenMode) {
        console.log(`üì§ Final output folder: ${path.resolve(CONFIG.finalOutputFolder)}`);
        console.log(`üîÄ Split-screen folders: ${path.resolve(CONFIG.splitscreenFolder)} + ${path.resolve(CONFIG.splitscreenSourceFolder)}`);
    } else {
    console.log(`üì§ Output folder: ${path.resolve(CONFIG.outputFolder)}`);
    }
    
    console.log('üéØ Supported audio formats:', CONFIG.supportedAudioFormats.join(', '));
    
    if (debugOverlay) {
        console.log('üîç DEBUG MODE: Text overlays will show clip information');
    }
    
    if (splitScreenMode) {
        console.log('üîÄ SPLIT-SCREEN MODE: Videos will include dynamic split-screen intro');
    }
    
    // Watch for audio files
    const audioWatcher = chokidar.watch(watchPath, {
        ignored: [
            /(^|[\/\\])\../ // ignore dotfiles
        ],
        persistent: true,
        ignoreInitial: true
    });
    
    audioWatcher.on('add', (filePath) => {
        const ext = path.extname(filePath).toLowerCase();
        if (CONFIG.supportedAudioFormats.includes(ext)) {
            console.log(`\nüÜï New audio file detected: ${path.basename(filePath)}`);
            
            // Wait a moment for file to be fully written
            setTimeout(() => {
                processAudioFile(filePath, debugOverlay, splitScreenMode);
            }, 1000);
        }
    });
    
    // Watch for status files (coordination with API)
    const statusWatcher = chokidar.watch(statusPath, {
        ignored: [
            /(^|[\/\\])\../ // ignore dotfiles
        ],
        persistent: true,
        ignoreInitial: true
    });
    
    statusWatcher.on('add', (statusFilePath) => {
        if (path.extname(statusFilePath).toLowerCase() === '.json') {
            console.log(`\nüìã New status file detected: ${path.basename(statusFilePath)}`);
            
            // Wait a moment for file to be fully written
            setTimeout(() => {
                processStatusFile(statusFilePath, debugOverlay, splitScreenMode);
            }, 500);
        }
    });
    
    audioWatcher.on('error', (error) => {
        console.error('‚ùå Audio watcher error:', error);
    });
    
    statusWatcher.on('error', (error) => {
        console.error('‚ùå Status watcher error:', error);
    });
    
    console.log('\n‚úÖ Auto Video Generator is running!');
    console.log('üí° Watching audiooutput/done folder for processed audio files');
    console.log('üí° Watching status folder for API coordination');
    console.log('üí° Generate audio with chunk_clone.py to trigger video creation');
    
    if (splitScreenMode) {
        console.log('üîÄ Split-screen videos will be saved to FinalOutput folder');
    }
    
    console.log('üõë Press Ctrl+C to stop\n');
}

// Parse command line arguments
const argv = minimist(process.argv.slice(2));
const debugOverlay = argv['debug-overlay'] || false;
const splitScreenMode = argv['split'] || false;
const processFile = argv['process-file'] || argv['file'];
const showHelp = argv.help || argv.h;

// Show help if requested
if (showHelp) {
    console.log(`
üé¨ Auto Video Generator - Automated video creation from audio files

USAGE:
  node auto-video-generator.js [OPTIONS]

OPTIONS:
  --debug-overlay           Enable debug text overlay showing clip information
  --split                   Enable split-screen mode with intelligent cropping
  --process-file <path>     Process a specific audio file manually (can use --file as shorthand)
  --help, -h               Show this help message

EXAMPLES:
  node auto-video-generator.js
    Start watching for audio files (normal mode)
    
  node auto-video-generator.js --debug-overlay
    Start with debug overlays enabled - shows clip name, durations, and transitions
    
  node auto-video-generator.js --split
    Start with split-screen mode - adds 4s split-screen intro before normal video
    
  node auto-video-generator.js --split --debug-overlay
    Start with both split-screen mode and debug overlays enabled
    
  node auto-video-generator.js --process-file "../zero-wire/Spark-TTS/audiooutput/done/audio_20241206.wav" --split
    Manually process a specific audio file with split-screen mode (no folder watching)
    
  node auto-video-generator.js --file "audio.wav" --split --debug-overlay
    Process specific file with split-screen and debug overlays (shorthand version)
    
DESCRIPTION:
  Watches ../zero-wire/Spark-TTS/audiooutput/done/ for new audio files and 
  automatically generates videos using templates from ../VideoTemplates/style 1/
  
  Normal mode: Creates videos with Intro ‚Üí Loop(s) ‚Üí End structure
  Split-screen mode: Creates videos with Split-screen (4s) ‚Üí Intro ‚Üí Loop(s) ‚Üí End structure
  
  Split-screen videos use:
  - Random video from ../VideoTemplates/style 1/splitscreen/ (top half)
  - Random video from ../splitscreensource/ (bottom half) 
  
  INTELLIGENT CROPPING:
  - If splitscreensource video height ‚â• 50% of loop template height: CROP both videos
    * Both videos cropped to middle 50% (remove 25% from top and bottom)
    * Stacked vertically to create split-screen effect
  - If splitscreensource video height < 50% of loop template height: POSITION without cropping
    * Top half: splitscreen template scaled to fit
    * Bottom half: splitscreensource positioned at 25% from top (for top) and 75% from top (for bottom)
    * Measured from center of splitscreensource video
  
  Final videos saved to ../FinalOutput/ folder
  
  Debug overlay format: "CLIP_NAME | Full: Xs | Used: Ys | Trans: none"
  - CLIP_NAME: SPLITSCREEN, INTRO, LOOP1, LOOP2, etc., or END
  - Full: Original video file duration
  - Used: Actual duration used in timeline (may be trimmed for end clip)
  - Trans: Transition type applied (always "none" for seamless cuts)
    
  MANUAL PROCESSING:
  - Use --process-file to manually process a specific audio file
  - Audio will be synchronized only with main video content (not split-screen intro)
  - If main video is shorter than audio, audio will be ignored
  - Ideal for processing files that weren't picked up by folder watching
`);
    process.exit(0);
}

// Start the watcher or process specific file
if (require.main === module) {
    if (processFile) {
        // Manual file processing mode
        console.log('üìÅ Manual file processing mode');
        
        // Resolve the file path
        const filePath = path.resolve(processFile);
        
        if (!fs.existsSync(filePath)) {
            console.error(`‚ùå Audio file not found: ${filePath}`);
            process.exit(1);
        }
        
        // Check if it's a supported audio format
        const ext = path.extname(filePath).toLowerCase();
        if (!CONFIG.supportedAudioFormats.includes(ext)) {
            console.error(`‚ùå Unsupported audio format: ${ext}`);
            console.error(`üéØ Supported formats: ${CONFIG.supportedAudioFormats.join(', ')}`);
            process.exit(1);
        }
        
        console.log(`üéµ Processing file: ${filePath}`);
        
        if (debugOverlay) {
            console.log('üîç Debug overlay enabled');
        }
        if (splitScreenMode) {
            console.log('üîÄ Split-screen mode enabled');
        }
        
        // Process the file directly
        processAudioFile(filePath, debugOverlay, splitScreenMode)
            .then((outputPath) => {
                if (outputPath) {
                    console.log(`\n‚úÖ Processing complete!`);
                    console.log(`üìÅ Output: ${outputPath}`);
                } else {
                    console.log(`\n‚ö†Ô∏è Processing completed but no output generated`);
                }
                process.exit(0);
            })
            .catch((error) => {
                console.error(`\n‚ùå Processing failed:`, error.message);
                process.exit(1);
            });
            
    } else {
        // Normal file watching mode
    if (debugOverlay) {
        console.log('üîç Starting with debug overlay enabled');
    }
        if (splitScreenMode) {
            console.log('üîÄ Starting with split-screen mode enabled');
        }
        initializeWatcher(debugOverlay, splitScreenMode);
    }
}

/**
 * Generate split-screen video clip with intelligent cropping based on source video height
 */
// FFmpeg-based generateSplitScreenClip function removed - using Editly exclusively

// Old generateSplitScreenEditlyConfig function removed - using new Editly-only approach

module.exports = {
    processAudioFile,
    getMediaDuration,
    getVideoDimensions,
    calculateVideoStructure,
    generateEditlyConfig,
    generateSplitScreenVideo,
    generateNewSplitScreenConfig
};